{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from random import randint\n",
    "import re\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Data: Get financial data for S&P 500 stocks for the past 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp500 = pd.read_excel('./S&P500_Stocks.xlsx')\n",
    "tickers = sp500.Ticker.tolist()\n",
    "#print(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the Financial data from http://stockrow.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for stock in tickers[0]: \n",
    "    \n",
    "    # Get the data from https://stockrow.com\n",
    "    #print (stock, ' starting download')\n",
    "    df_income = pd.read_excel('https://stockrow.com/api/companies/'+str(stock)+'/financials.xlsx?dimension=MRQ&section=Income%20Statement')\n",
    "    df_balanceSheet = pd.read_excel('https://stockrow.com/api/companies/'+str(stock)+'/financials.xlsx?dimension=MRQ&section=Balance%20Sheet')\n",
    "    df_cash = pd.read_excel('https://stockrow.com/api/companies/'+str(stock)+'/financials.xlsx?dimension=MRQ&section=Cash%20Flow')\n",
    "    df_metrics = pd.read_excel('https://stockrow.com/api/companies/'+str(stock)+'/financials.xlsx?dimension=MRQ&section=Metrics')\n",
    "    df_growth = pd.read_excel('https://stockrow.com/api/companies/'+str(stock)+'/financials.xlsx?dimension=MRQ&section=Growth')\n",
    "    #print (stock, ' downloaded.')\n",
    "    \n",
    "    # Write to Excel\n",
    "    fname = str(stock) + \".xlsx\"\n",
    "    writer = pd.ExcelWriter(fname)\n",
    "    \n",
    "    df_income.to_excel(writer, sheet_name =\"income\")\n",
    "    df_balanceSheet.to_excel(writer, sheet_name =\"balance_sheet\")\n",
    "    df_cash.to_excel(writer, sheet_name =\"cash_flow\")\n",
    "    df_metrics.to_excel(writer, sheet_name =\"metrics\")\n",
    "    df_growth.to_excel(writer, sheet_name =\"growth\")\n",
    "    #print (stock, ' saved.')\n",
    "    \n",
    "    # Prevent DDOS\n",
    "    random_time = randint(5,15)\n",
    "    #print ('waiting, ', random_time,'secs')\n",
    "    time.sleep(random_time)\n",
    "\n",
    "    \n",
    "print('~~~Download Complete~~~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create consolidated lists of Income Sheet, Balance Sheet, cash flow, metrics, growth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('./Data') #directory with the stock files\n",
    "\n",
    "# Consolidated 'MasterList' DataFrames for each category\n",
    "income_df = pd.DataFrame() \n",
    "balanceSheet_df = pd.DataFrame() \n",
    "cashFlow_df = pd.DataFrame() \n",
    "metrics_df = pd.DataFrame() \n",
    "growth_df = pd.DataFrame() \n",
    "\n",
    "\n",
    "datasets = [income_df, balanceSheet_df, cashFlow_df, metrics_df, growth_df]\n",
    "sheet_names = ['income', 'balance_sheet', 'cash_flow', 'metrics', 'growth' ]\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    ticker = f.split('.')[0] # get stock ticker from filename (eg. AAPL.xlsx)\n",
    "    filepath = os.getcwd() + '\\\\Data\\\\'+f # file path for each stock file\n",
    "    \n",
    "    # For each file (eg. AAPL.xlsx), iterate through its tabs (eg. income, balance_sheet etc) and save into the 'Master List'\n",
    "    for i in range (5):\n",
    "        temp_df= pd.read_excel(filepath, sheetname = sheet_names[i]).transpose()\n",
    "        temp_df['Ticker'] = ticker\n",
    "        datasets[i] = pd.concat([datasets[i], temp_df])# Append stock data into Master List\n",
    "\n",
    "        \n",
    "# Assign values into \n",
    "income_df = datasets[0]\n",
    "balanceSheet_df =  datasets[1]\n",
    "cashFlow_df =  datasets[2]\n",
    "metrics_df =  datasets[3]\n",
    "growth_df =  datasets[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get historical prices for S&P500 stocks and the S&P index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function that will scrape Yahoo Finance for the historical stock data, and return the results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_yahoo_ticker_data(ticker):\n",
    "    '''\n",
    "    This method scraps Historical stock data from  Yahoo Finance.\n",
    "    This method takes in stock ticker as input (eg. 'AAPL, 'AMZN') and ouputs a DataFrame with\n",
    "    histrorical stock prices from Jan-04-2008 to Jan-31-2018. \n",
    "    Source Credit for scraping Yahoo Finance: \n",
    "    https://github.com/bradlucas/get-yahoo-quotes-python/blob/master/get-yahoo-quotes.py\n",
    "\n",
    "    '''\n",
    "    # Scrape Yahoo Fiance\n",
    "    #---------------------------------------------------\n",
    "    res = requests.get('https://finance.yahoo.com/quote/' + ticker + '/history')\n",
    "    yahoo_cookie = res.cookies['B']\n",
    "    yahoo_crumb = None\n",
    "    pattern = re.compile('.*\"CrumbStore\":\\{\"crumb\":\"(?P<crumb>[^\"]+)\"\\}')\n",
    "    for line in res.text.splitlines():\n",
    "        m = pattern.match(line)\n",
    "        if m is not None:\n",
    "            yahoo_crumb = m.groupdict()['crumb']\n",
    "    cookie_tuple = yahoo_cookie, yahoo_crumb\n",
    "\n",
    "    current_date = int(time.time()) # Can set your own custom end time.\n",
    "    url_kwargs = {'symbol': ticker, 'timestamp_end': current_date,\n",
    "        'crumb': cookie_tuple[1]}\n",
    "    #set period1= 0 for Max History, or enter custom unix date\n",
    "    #set interval=1d for daily updates, =1wk for weekly updates, =1mo for monthly updates\n",
    "    url_price = 'https://query1.finance.yahoo.com/v7/finance/download/' \\\n",
    "                '{symbol}?period1=1199163600&period2={timestamp_end}&interval=1d&events=history' \\\n",
    "                '&crumb={crumb}'.format(**url_kwargs)\n",
    "\n",
    "    time.sleep(1)\n",
    "    response = requests.get(url_price, cookies={'B': cookie_tuple[0]}) #webpage\n",
    "    \n",
    "    # Create pandas data frame from the downloaded page\n",
    "    #---------------------------------------------------\n",
    "    s = response.content\n",
    "    s = s.decode('utf-8')\n",
    "    s = s.replace('Adj Close', 'Adj_Close')\n",
    "    s_rows = s.split('\\n')\n",
    "    s_rows_cols = [each.split() for each in s_rows]\n",
    "    #print (s_rows_cols[:2])\n",
    "    header_row = ['Date','Open','High','Low','Close','Adj_Close','Volume']\n",
    "    df = pd.DataFrame (s_rows_cols[1:])\n",
    "    df = pd.concat([df[0].str.split(',', expand=True)], axis=1)\n",
    "    df.columns = header_row\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `stock_price` DataFrame, which is a consolidated dataset of the historical stock prices for S&P500 stocks (From: Jan 1, 2008 to Jan 31, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missed_stocks=[] # Capture any missed stocks due to error while scrpaing Yahoo Finance\n",
    "stock_prices = pd.DataFrame({'Tickers':[]})\n",
    "\n",
    "for stock in tickers[0:10]:\n",
    "    print ('Stock:', stock)\n",
    "    try: #if we can successfully get the stock price data\n",
    "        current_stock = get_yahoo_ticker_data(stock)\n",
    "        current_stock['Ticker'] = stock\n",
    "        stock_prices = pd.concat([stock_prices, current_stock])\n",
    "          \n",
    "    except Exception as ex:# generate list of stocks for which we were unable to obtain stock data\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print (message)\n",
    "       \n",
    "        missed_stocks.append(stock)\n",
    "    #time.sleep(1)\n",
    "\n",
    "print (missed_stocks) # Note: re-run the above script with these missed stocks as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the S&P 500 index (ticker: ^GSPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: #if we can successfully get the stock price data\n",
    "    sp500 = get_yahoo_ticker_data('^GSPC')\n",
    "    sp500['Ticker'] = 'S&P500'\n",
    "    stock_prices = pd.concat([stock_prices, sp500])\n",
    "    \n",
    "except Exception as ex:# generate list of stocks for which we were unable to obtain stock data\n",
    "        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "        message = template.format(type(ex).__name__, ex.args)\n",
    "        print (message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Optional) Save to excel and to pickle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_df.to_excel('income.xlsx')\n",
    "balanceSheet_df.to_excel('balanceSheet.xlsx')\n",
    "cashFlow_df.to_excel('cashFlow.xlsx')\n",
    "metrics_df.to_excel('metrics.xlsx')\n",
    "growth_df.to_excel('growth.xlsx')\n",
    "#growth_df.to_excel('stock_prices_v5.xlsx') # large file 100+MB\n",
    "\n",
    "\n",
    "income_df.to_pickle('income.pkl')\n",
    "balanceSheet_df.to_pickle('balanceSheet.pkl')\n",
    "cashFlow_df.to_pickle('cashFlow.pkl')\n",
    "metrics_df.to_pickle('metrics.pkl')\n",
    "growth_df.to_pickle('growth.pkl')\n",
    "stock_prices.to_pickle('stock_prices_v5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Data Wrangling: Building the Dataset for Analysis\n",
    "\n",
    "To create our stock buy/sell prediction model, we will look at the following financial metrics (features):\n",
    "https://www.investopedia.com/articles/fundamental-analysis/09/five-must-have-metrics-value-investors.asp\n",
    "1. `Price-to-Earnings Ratio (P/E)` : calculated by Market Price per share / Earnings per Share (annual)\n",
    "2. `Price-to-Book Ratio (P/B)` : calculated by Market Price per share / Book Value per Share  \n",
    "3. `Debt-Equity Ratio (D/E)` : available in the metrics dataframe\n",
    "4. `Free Cash Flow` : available in the cashFlow dataframe\n",
    "5. `Price/Earnings to Growth Ratio (PEG)` : calculate by P/E ratio / EPS growth in the period\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Features:\n",
    "- Stock \n",
    "- Industry\n",
    "- Date \n",
    "- Stock Price close \n",
    "- %Change Stock \n",
    "- 5 ratios (listed above)\n",
    "- SP500 Price \n",
    "- %Change SP500 \n",
    "\n",
    "\n",
    "Response:\n",
    "- Buy/Sell label: based on whether the stock price outperformed/underperfomed the S&P 500 index in the quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Optional) Read dataframes from pickle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_df = pd.read_pickle('income.pkl')\n",
    "balanceSheet_df = pd.read_pickle('balanceSheet.pkl')\n",
    "cashFlow_df = pd.read_pickle('cashFlow.pkl')\n",
    "metrics_df = pd.read_pickle('metrics.pkl')\n",
    "growth_df= pd.read_pickle('growth.pkl')\n",
    "stock_prices = pd.read_pickle('stock_prices.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name for index. This will be useful later on when we reset index to extract date as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_df.index.name = 'Date'\n",
    "balanceSheet_df.index.name = 'Date'\n",
    "cashFlow_df.index.name = 'Date'\n",
    "metrics_df.index.name = 'Date'\n",
    "growth_df.index.name = 'Date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick examination of the Dtypes of the index reveals that there are some non-date index in the `income_df`, `metrics_df`, and `growth_df`. There are few entries with dates such as, \"Unnamed: 19\" or \"Unnamed: 20\", which can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype of income_df index:  object\n",
      "Dtype of balanceSheet_df index:  datetime64[ns]\n",
      "Dtype of cashFlow_df index:  datetime64[ns]\n",
      "Dtype of metrics_df index:  object\n",
      "Dtype of growth_df index:  object\n"
     ]
    }
   ],
   "source": [
    "print ('Dtype of income_df index: ', income_df.index.dtype)\n",
    "print ('Dtype of balanceSheet_df index: ', balanceSheet_df.index.dtype)\n",
    "print ('Dtype of cashFlow_df index: ', cashFlow_df.index.dtype)\n",
    "print ('Dtype of metrics_df index: ', metrics_df.index.dtype)\n",
    "print ('Dtype of growth_df index: ', growth_df.index.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional Code: Run this code to see the number of rows that have 'Unnamed' index. Change the name of the dataframe \n",
    "\n",
    "counter = 0\n",
    "for i in range (len(growth_df.index)): # iterate through the dataframe index\n",
    "    # if the index type is a String, then it is not a date\n",
    "    if type(growth_df.index[i]) is str:\n",
    "        print ('i: ', i, 'index: ', growth_df.index[i]) # debug print\n",
    "        counter += 1\n",
    "print ('Total # of rows with \"unnamed\" index: ', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change index to type DateTime. Passing in the argument `errors='coerce'` will change all non-date values (eg. 'Unnamed: 20') to 'NaT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change index to type DateTime. Add in errors='coerce' will change all non-date values to 'NaT'\n",
    "income_df.index = pd.to_datetime(income_df.index, errors = 'coerce') \n",
    "metrics_df.index = pd.to_datetime(metrics_df.index, errors = 'coerce')\n",
    "growth_df.index = pd.to_datetime(growth_df.index, errors = 'coerce')\n",
    "\n",
    "# Drop rows that have 'NaT' type values\n",
    "income_df = income_df[pd.isnull(income_df.index) == False]\n",
    "metrics_df = metrics_df[pd.isnull(metrics_df.index) == False]\n",
    "growth_df = growth_df[pd.isnull(growth_df.index) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset index to bring date as a Feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_df.reset_index(inplace = True)\n",
    "balanceSheet_df.reset_index(inplace = True)\n",
    "cashFlow_df.reset_index(inplace = True)\n",
    "metrics_df.reset_index(inplace = True)\n",
    "growth_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review shape of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18771, 31)\n",
      "(18679, 28)\n",
      "(18752, 16)\n",
      "(18649, 23)\n",
      "(18585, 15)\n"
     ]
    }
   ],
   "source": [
    "# Review shape of DataFrames\n",
    "print (income_df.shape)\n",
    "print (balanceSheet_df.shape)\n",
    "print (cashFlow_df.shape)\n",
    "print (metrics_df.shape)\n",
    "print (growth_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a consolidate DataFrame, called `all_df`, that combines the data from of the other 5 datasets. Later on, we will also port in the S&P500 price info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18817, 105)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Consolidated_Income</th>\n",
       "      <th>Cost_of_Revenue</th>\n",
       "      <th>Dividends_per_Basic_Common_Share</th>\n",
       "      <th>EBIT</th>\n",
       "      <th>EBIT_Margin</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>EBITDA_Margin</th>\n",
       "      <th>EPS</th>\n",
       "      <th>EPS_Diluted</th>\n",
       "      <th>...</th>\n",
       "      <th>Dividends_per_Basic_Common_Share_Growth</th>\n",
       "      <th>EBIT_Growth</th>\n",
       "      <th>EPS_Diluted_Growth</th>\n",
       "      <th>EPS_Growth</th>\n",
       "      <th>Gross_Profit_Growth</th>\n",
       "      <th>Inventory_Growth</th>\n",
       "      <th>Net_Income_Growth</th>\n",
       "      <th>Operating_Cash_Flow_Growth</th>\n",
       "      <th>Trade_and_Non-Trade_Receivables_Growth</th>\n",
       "      <th>Weighted_Average_Shares_Diluted_Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>177000000.0</td>\n",
       "      <td>542000000.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>246000000.0</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>298000000.0</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.4471</td>\n",
       "      <td>0.4211</td>\n",
       "      <td>0.4103</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>175000000.0</td>\n",
       "      <td>518000000.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>212000000.0</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>263000000.0</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>0.4211</td>\n",
       "      <td>0.4474</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>0.4113</td>\n",
       "      <td>0.1753</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>-0.0061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>164000000.0</td>\n",
       "      <td>510000000.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>211000000.0</td>\n",
       "      <td>0.1915</td>\n",
       "      <td>265000000.0</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.5630</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.8214</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>-0.0126</td>\n",
       "      <td>0.8022</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>-0.0091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>168000000.0</td>\n",
       "      <td>493000000.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>213000000.0</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>268000000.0</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.4054</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>0.3884</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>-0.0181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>126000000.0</td>\n",
       "      <td>523000000.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>170000000.0</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>226000000.0</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>-0.0952</td>\n",
       "      <td>-0.0714</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>-0.0290</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>-0.0090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Consolidated_Income  Cost_of_Revenue  \\\n",
       "0 2017-10-31          177000000.0      542000000.0   \n",
       "1 2017-07-31          175000000.0      518000000.0   \n",
       "2 2017-04-30          164000000.0      510000000.0   \n",
       "3 2017-01-31          168000000.0      493000000.0   \n",
       "4 2016-10-31          126000000.0      523000000.0   \n",
       "\n",
       "   Dividends_per_Basic_Common_Share         EBIT  EBIT_Margin       EBITDA  \\\n",
       "0                              0.13  246000000.0       0.2069  298000000.0   \n",
       "1                              0.13  212000000.0       0.1903  263000000.0   \n",
       "2                              0.13  211000000.0       0.1915  265000000.0   \n",
       "3                              0.13  213000000.0       0.1996  268000000.0   \n",
       "4                              0.12  170000000.0       0.1530  226000000.0   \n",
       "\n",
       "   EBITDA_Margin   EPS  EPS_Diluted                   ...                    \\\n",
       "0          0.251  0.55         0.54                   ...                     \n",
       "1          0.236  0.55         0.54                   ...                     \n",
       "2          0.240  0.51         0.50                   ...                     \n",
       "3          0.251  0.52         0.52                   ...                     \n",
       "4          0.203  0.39         0.38                   ...                     \n",
       "\n",
       "   Dividends_per_Basic_Common_Share_Growth  EBIT_Growth  EPS_Diluted_Growth  \\\n",
       "0                                   0.0833       0.4471              0.4211   \n",
       "1                                   0.0833       0.4040              0.4211   \n",
       "2                                   0.0833       0.5630              0.7857   \n",
       "3                                   0.0833       0.3313              0.4444   \n",
       "4                                   0.2000       0.0692             -0.0952   \n",
       "\n",
       "   EPS_Growth  Gross_Profit_Growth  Inventory_Growth  Net_Income_Growth  \\\n",
       "0      0.4103               0.1003            0.0788             0.4048   \n",
       "1      0.4474               0.0996            0.0424             0.4113   \n",
       "2      0.8214               0.1170           -0.0126             0.8022   \n",
       "3      0.4054               0.0689           -0.0054             0.3884   \n",
       "4     -0.0714               0.0991           -0.0148            -0.1000   \n",
       "\n",
       "   Operating_Cash_Flow_Growth  Trade_and_Non-Trade_Receivables_Growth  \\\n",
       "0                      0.2308                                  0.1474   \n",
       "1                      0.1753                                  0.1492   \n",
       "2                      0.0118                                  0.1246   \n",
       "3                      0.0450                                  0.0583   \n",
       "4                     -0.0290                                  0.0413   \n",
       "\n",
       "   Weighted_Average_Shares_Diluted_Growth  \n",
       "0                                  0.0000  \n",
       "1                                 -0.0061  \n",
       "2                                 -0.0091  \n",
       "3                                 -0.0181  \n",
       "4                                 -0.0090  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge the 5 datasets\n",
    "datasets = [income_df, balanceSheet_df, cashFlow_df, metrics_df, growth_df]\n",
    "all_df = datasets[0].copy()\n",
    "for df in datasets[1:]:\n",
    "    all_df = pd.merge (all_df, df, on=['Date','Ticker'], how = 'outer')\n",
    "\n",
    "# replace spaces in column names with '_'\n",
    "all_df.columns = all_df.columns.str.replace(' ','_') \n",
    "\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add in the S&P 500 stock price data. \n",
    "\n",
    "We will have to pay particular attention to the `date` while doing the lookup. The financial datasets (now combined in `all_df`) have dates that fall on the weekend. We will have to correct the dates to reflect the previous business day before doing the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1229684 entries, 0 to 2540\n",
      "Data columns (total 8 columns):\n",
      "Adj_Close    1229178 non-null object\n",
      "Close        1229178 non-null object\n",
      "Date         1229178 non-null datetime64[ns]\n",
      "High         1229178 non-null object\n",
      "Low          1229178 non-null object\n",
      "Open         1229178 non-null object\n",
      "Ticker       1229684 non-null object\n",
      "Volume       1229178 non-null object\n",
      "dtypes: datetime64[ns](1), object(7)\n",
      "memory usage: 84.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert the Date in stock_prices to Datetime\n",
    "stock_prices.Date = pd.to_datetime(stock_prices.Date, errors = 'coerce')\n",
    "stock_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We will replace the Date column with a modified version that accounts for Holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_old</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-31</td>\n",
       "      <td>2017-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>2017-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-30</td>\n",
       "      <td>2017-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>2017-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>2016-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Date_old       Date\n",
       "0 2017-10-31 2017-10-31\n",
       "1 2017-07-31 2017-07-31\n",
       "2 2017-04-30 2017-04-28\n",
       "3 2017-01-31 2017-01-31\n",
       "4 2016-10-31 2016-10-31"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.rename(columns={'Date':'Date_old'}, inplace = True)\n",
    "\n",
    "# Apply formula to update date\n",
    "from pandas.tseries.offsets import BDay\n",
    "all_df['Date'] = all_df['Date_old'].apply(lambda row: row if row.dayofweek in (0,1,2,3,4) else row-BDay(1))\n",
    "\n",
    "# Review our changes\n",
    "all_df[['Date_old','Date']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the 3rd entry, the formula changed the date \n",
    "\n",
    "    From: 2017-04-30 (Sun)    \n",
    "    To:   2017-04-28 (Fri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's merge the S&P 500 data from `stock_prices` to `all_df`. We will also add a column to `all_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18817, 112)\n"
     ]
    }
   ],
   "source": [
    "all_df = pd.merge(all_df, stock_prices, on=['Ticker','Date'], how ='left')\n",
    "print(all_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets add a `SP500` column that has the S&P500 index prices for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18817, 113)\n"
     ]
    }
   ],
   "source": [
    "#Subset the stock prices DataFrame for the dates in all_df (ie. quarterly financials dates)\n",
    "sp500 = stock_prices.loc[(stock_prices.Ticker == 'S&P500'),['Date','Close']]\n",
    "sp500.rename(columns = {'Close':'SP500'}, inplace = True)\n",
    "\n",
    "# Merge the data\n",
    "all_df = pd.merge(all_df, sp500, on=['Date'], how ='left')\n",
    "print(all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date_old', 'Consolidated_Income', 'Cost_of_Revenue',\n",
      "       'Dividends_per_Basic_Common_Share', 'EBIT', 'EBIT_Margin', 'EBITDA',\n",
      "       'EBITDA_Margin', 'EPS', 'EPS_Diluted',\n",
      "       ...\n",
      "       'Trade_and_Non-Trade_Receivables_Growth',\n",
      "       'Weighted_Average_Shares_Diluted_Growth', 'Date', 'Adj_Close', 'Close',\n",
      "       'High', 'Low', 'Open', 'Volume', 'SP500'],\n",
      "      dtype='object', length=113)\n"
     ]
    }
   ],
   "source": [
    "# Columns\n",
    "print(all_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Optional) Save all our changes to pickle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_df.to_pickle('income_v2.pkl')\n",
    "balanceSheet_df.to_pickle('balanceSheet_v2.pkl')\n",
    "cashFlow_df.to_pickle('cashFlow_v2.pkl')\n",
    "metrics_df.to_pickle('metrics_v2.pkl')\n",
    "growth_df.to_pickle('growth_v2.pkl')\n",
    "stock_prices.to_pickle('stock_prices_v2.pkl')\n",
    "all_df.to_pickle('all_df_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Optional) Read dataframes from pickle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_df = pd.read_pickle('income_v2.pkl')\n",
    "balanceSheet_df = pd.read_pickle('balanceSheet_v2.pkl')\n",
    "cashFlow_df = pd.read_pickle('cashFlow_v2.pkl')\n",
    "metrics_df =pd.read_pickle('metrics_v2.pkl')\n",
    "growth_df = pd.read_pickle('growth_v2.pkl')\n",
    "stock_prices = pd.read_pickle('stock_prices_v2.pkl')\n",
    "all_df.to_pickle('all_df_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
